****************************************ARIMA*********************************
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from statsmodels.tsa.arima.model import ARIMA
from pmdarima import auto_arima
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings

warnings.filterwarnings('ignore')


def get_stock_data(ticker, start_date, end_date):
    """è·å–è‚¡ç¥¨å†å²æ•°æ®"""
    print(f"\næ­£åœ¨ä¸‹è½½ {ticker} ä» {start_date} åˆ° {end_date} çš„æ•°æ®...")
    data = yf.download(ticker, start=start_date, end=end_date)
    print(f"è·å–åˆ° {len(data)} æ¡æ•°æ®")
    return data['Close']


def find_best_arima_params(data):
    """è‡ªåŠ¨å¯»æ‰¾æœ€ä½³ARIMAå‚æ•°"""
    print("\nå¼€å§‹è‡ªåŠ¨å¯»æ‰¾æœ€ä½³ARIMAå‚æ•°...")
    stepwise_fit = auto_arima(data,
                              start_p=0, max_p=5,
                              start_q=0, max_q=5,
                              m=1,
                              seasonal=False,
                              d=1, max_d=2,
                              trace=True,
                              error_action='ignore',
                              suppress_warnings=True,
                              stepwise=True)
    print(f"æ‰¾åˆ°æœ€ä½³å‚æ•°ç»„åˆ: {stepwise_fit.order}")
    return stepwise_fit.order


def train_and_predict_arima(data, prediction_steps, order):
    """è®­ç»ƒARIMAæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹"""
    print(f"\nä½¿ç”¨ARIMA{order}è¿›è¡Œè®­ç»ƒ...")
    model = ARIMA(data, order=order)
    results = model.fit()
    print("\næ¨¡å‹è®­ç»ƒå®Œæˆï¼Œå¼€å§‹é¢„æµ‹...")
    forecast = results.forecast(steps=prediction_steps)
    return forecast.values


def evaluate_predictions(actual, predicted):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    actual = np.asarray(actual).ravel()
    predicted = np.asarray(predicted).ravel()

    min_len = min(len(actual), len(predicted))
    actual = actual[:min_len]
    predicted = predicted[:min_len]

    mse = mean_squared_error(actual, predicted)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actual, predicted)
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100
    return mse, rmse, mae, mape


def plot_predictions(dates, actual, predicted, title, save_path):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    plt.figure(figsize=(12, 6))
    plt.plot(dates, actual, label='Actual Price', color='green')
    plt.plot(dates, predicted, label='Predicted Price', color='red', linestyle='--')
    plt.title(title)
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.show()


def save_results(dates, actual, predicted, filepath):
    """ä¿å­˜é¢„æµ‹ç»“æœ"""
    results_df = pd.DataFrame({
        'Date': [d.strftime('%Y-%m-%d') if isinstance(d, pd.Timestamp) else d for d in dates],
        'Actual_Price': np.asarray(actual).ravel(),
        'Predicted_Price': np.asarray(predicted).ravel()
    })
    results_df.to_csv(filepath, index=False)
    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {filepath}")
    return results_df


if __name__ == "__main__":
    # å‚æ•°è®¾ç½®
    ticker = "GOOG"
    prediction_start = "2024-12-02"
    end_date = "2024-12-31"

    periods = {
        '30days': 30,
        '6months': 180,
        '3years': 1095
    }

    # åˆ›å»ºä¸€ä¸ªDataFrameæ¥å­˜å‚¨æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„è¯„ä¼°æŒ‡æ ‡
    metrics_df = pd.DataFrame(columns=['Period', 'MSE', 'RMSE', 'MAE', 'MAPE'])

    for period_name, days in periods.items():
        print(f"\n{'=' * 50}")
        print(f"å¼€å§‹è®­ç»ƒ {period_name} å†å²æ•°æ®çš„ARIMAæ¨¡å‹:")
        print(f"{'=' * 50}")

        # è®¡ç®—èµ·å§‹æ—¥æœŸå¹¶è·å–æ•°æ®
        start_date = (pd.to_datetime(prediction_start) - pd.Timedelta(days=days)).strftime('%Y-%m-%d')
        train_data = get_stock_data(ticker, start_date, prediction_start)

        # è·å–å®é™…æœªæ¥æ•°æ®
        future_dates = pd.date_range(start=prediction_start, end=end_date, freq='B')
        actual_data = get_stock_data(ticker, prediction_start, end_date)
        actual_data = actual_data.reindex(future_dates).ffill()

        # æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹
        best_order = find_best_arima_params(train_data)
        predictions = train_and_predict_arima(train_data, len(future_dates), best_order)

        # æ•°æ®å¯¹é½
        min_len = min(len(predictions), len(actual_data))
        predictions = predictions[:min_len]
        actual_values = actual_data.values[:min_len]
        dates = future_dates[:min_len]

        # ä¿å­˜é¢„æµ‹ç»“æœ
        base_path = f'/Users/quyou/Desktop/æ¯•ä¸šè®¾è®¡/forecast data/ARIMA/GOOG_arima_{period_name}'
        results_df = save_results(
            dates,
            actual_values,
            predictions,
            f'{base_path}.csv'
        )

        # ç»˜åˆ¶ç»“æœ
        plot_predictions(
            dates,
            actual_values,
            predictions,
            f'ARIMA Stock Price Prediction ({period_name} Training)',
            f'{base_path}_plot.png'
        )

        # è®¡ç®—å¹¶æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡
        mse, rmse, mae, mape = evaluate_predictions(actual_values, predictions)
        print(f"\næ¨¡å‹è¯„ä¼°æŒ‡æ ‡ ({period_name}):")
        print(f"MSE: {mse:.2f}")
        print(f"RMSE: {rmse:.2f}")
        print(f"MAE: {mae:.2f}")
        print(f"MAPE: {mape:.2f}%")

        # å°†è¯„ä¼°æŒ‡æ ‡æ·»åŠ åˆ°DataFrame
        metrics_df = pd.concat([metrics_df, pd.DataFrame({
            'Period': [period_name],
            'MSE': [mse],
            'RMSE': [rmse],
            'MAE': [mae],
            'MAPE': [mape]
        })])

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœæ ·ä¾‹
        print("\né¢„æµ‹ç»“æœç¤ºä¾‹:")
        print("\nå‰5æ¡è®°å½•:")
        print(results_df.head())
        print("\nå5æ¡è®°å½•:")
        print(results_df.tail())

    # ä¿å­˜æ‰€æœ‰æ—¶é—´å‘¨æœŸçš„è¯„ä¼°æŒ‡æ ‡
    metrics_df.to_csv('/Users/quyou/Desktop/æ¯•ä¸šè®¾è®¡/forecast data/ARIMA/GOOG_arima_metrics.csv', index=False)
    print("\næ‰€æœ‰è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³: GOOG_arima_metrics.csv")













*************************************GARCH*******************************************












import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import yfinance as yf
from arch import arch_model
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings

warnings.filterwarnings('ignore')


def get_stock_data(ticker, start_date, end_date):
    """è·å–è‚¡ç¥¨å†å²æ•°æ®"""
    print(f"\næ­£åœ¨ä¸‹è½½ {ticker} ä» {start_date} åˆ° {end_date} çš„æ•°æ®...")
    data = yf.download(ticker, start=start_date, end=end_date)
    print(f"è·å–åˆ° {len(data)} æ¡æ•°æ®")
    return data['Close']


def calculate_returns(prices):
    """è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡"""
    returns = np.log(prices / prices.shift(1)).dropna()
    # ç¡®ä¿è¿”å›ä¸€ç»´æ•°ç»„
    if isinstance(returns, pd.Series):
        returns = returns.values
    if isinstance(returns, pd.DataFrame):
        returns = returns.values
    return returns.flatten()


def evaluate_predictions(actual, predicted):
    """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
    actual = np.asarray(actual).ravel()
    predicted = np.asarray(predicted).ravel()

    min_len = min(len(actual), len(predicted))
    actual = actual[:min_len]
    predicted = predicted[:min_len]

    mse = mean_squared_error(actual, predicted)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(actual, predicted)
    mape = np.mean(np.abs((actual - predicted) / actual)) * 100
    return mse, rmse, mae, mape


def train_and_predict_garch(train_data, last_price, prediction_steps):
    """
    è®­ç»ƒGARCHæ¨¡å‹å¹¶è¿›è¡Œé¢„æµ‹
    @param train_data: è®­ç»ƒæ•°æ®çš„æ”¶ç›Šç‡
    @param last_price: æœ€åä¸€ä¸ªä»·æ ¼
    @param prediction_steps: é¢„æµ‹æ­¥æ•°
    @return: é¢„æµ‹çš„ä»·æ ¼åºåˆ—
    """
    print("\nå¼€å§‹è®­ç»ƒGARCHæ¨¡å‹...")

    # ç¡®ä¿è®­ç»ƒæ•°æ®æ˜¯ä¸€ç»´æ•°ç»„
    if isinstance(train_data, pd.Series):
        train_data = train_data.values
    if isinstance(train_data, pd.DataFrame):
        train_data = train_data.values
    train_data = train_data.flatten()

    # è®¾ç½®GARCHæ¨¡å‹å‚æ•°
    model = arch_model(train_data, vol='Garch', p=1, q=1, mean='Zero', dist='normal')

    # è®­ç»ƒæ¨¡å‹
    print("æ­£åœ¨æ‹Ÿåˆæ¨¡å‹...")
    results = model.fit(disp='off')
    print("æ¨¡å‹è®­ç»ƒå®Œæˆ")

    # é¢„æµ‹æ³¢åŠ¨ç‡
    print("\nå¼€å§‹é¢„æµ‹...")
    forecasts = results.forecast(horizon=prediction_steps)
    conditional_vol = np.sqrt(forecasts.variance.values[-1, :])

    # ä½¿ç”¨è’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç”Ÿæˆä»·æ ¼è·¯å¾„
    np.random.seed(42)
    sim_returns = np.random.normal(train_data.mean(), 1) * conditional_vol

    # å°†æ”¶ç›Šç‡è½¬æ¢ä¸ºä»·æ ¼
    price_path = last_price * np.exp(np.cumsum(sim_returns))

    return price_path.flatten()


def plot_predictions(dates, actual, predicted, title, save_path):
    """ç»˜åˆ¶é¢„æµ‹ç»“æœ"""
    plt.figure(figsize=(12, 6))
    plt.plot(dates, actual, label='Actual Price', color='green')
    plt.plot(dates, predicted, label='Predicted Price', color='red', linestyle='--')
    plt.title(title)
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


def plot_comparison(all_predictions, save_path):
    """ç»˜åˆ¶ä¸‰ä¸ªæ—¶é—´å‘¨æœŸçš„å¯¹æ¯”å›¾"""
    plt.figure(figsize=(15, 8))

    colors = {'30days': 'red', '6months': 'blue', '3years': 'green'}
    linestyles = {'30days': '--', '6months': '-.', '3years': ':'}

    # ç»˜åˆ¶å®é™…å€¼
    actual_values = next(iter(all_predictions.values()))['actual']
    dates = next(iter(all_predictions.values()))['dates']
    plt.plot(dates, actual_values, label='Actual Price', color='black', linewidth=2)

    # ç»˜åˆ¶å„ä¸ªæ—¶é—´å‘¨æœŸçš„é¢„æµ‹å€¼
    for period, data in all_predictions.items():
        plt.plot(data['dates'], data['predicted'],
                 label=f'{period} Prediction',
                 color=colors[period],
                 linestyle=linestyles[period],
                 linewidth=1.5)

    plt.title('GARCH Predictions Comparison (Different Training Periods)')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.savefig(save_path)
    plt.close()


def save_results(dates, actual, predicted, filepath):
    """ä¿å­˜é¢„æµ‹ç»“æœ"""
    # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯ä¸€ç»´æ•°ç»„
    if isinstance(actual, pd.Series):
        actual = actual.values
    if isinstance(actual, pd.DataFrame):
        actual = actual.values
    actual = np.asarray(actual).ravel()

    if isinstance(predicted, pd.Series):
        predicted = predicted.values
    if isinstance(predicted, pd.DataFrame):
        predicted = predicted.values
    predicted = np.asarray(predicted).ravel()

    dates = pd.to_datetime(dates)

    # åˆ›å»ºæ—¥æœŸåˆ—è¡¨
    dates_list = [d.strftime('%Y-%m-%d') for d in dates]

    # ç¡®ä¿æ‰€æœ‰æ•°æ®é•¿åº¦ä¸€è‡´
    min_len = min(len(dates_list), len(actual), len(predicted))

    results_df = pd.DataFrame({
        'Date': dates_list[:min_len],
        'Actual_Price': actual[:min_len],
        'Predicted_Price': predicted[:min_len]
    })

    results_df.to_csv(filepath, index=False)
    print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜è‡³: {filepath}")
    return results_df


if __name__ == "__main__":
    # å‚æ•°è®¾ç½®
    ticker = "GOOG"
    prediction_start = "2024-12-02"
    end_date = "2024-12-31"

    periods = {
        '30days': 30,
        '6months': 180,
        '3years': 1095
    }

    # åˆ›å»ºDataFrameå­˜å‚¨è¯„ä¼°æŒ‡æ ‡
    metrics_df = pd.DataFrame(columns=['Period', 'MSE', 'RMSE', 'MAE', 'MAPE'])

    # å­˜å‚¨æ‰€æœ‰é¢„æµ‹ç»“æœç”¨äºå¯¹æ¯”å›¾
    all_predictions = {}

    for period_name, days in periods.items():
        print(f"\n{'=' * 50}")
        print(f"å¼€å§‹è®­ç»ƒ {period_name} å†å²æ•°æ®çš„GARCHæ¨¡å‹:")
        print(f"{'=' * 50}")

        try:
            # è®¡ç®—èµ·å§‹æ—¥æœŸå¹¶è·å–æ•°æ®
            start_date = (pd.to_datetime(prediction_start) - pd.Timedelta(days=days)).strftime('%Y-%m-%d')
            train_prices = get_stock_data(ticker, start_date, prediction_start)

            # è®¡ç®—æ”¶ç›Šç‡
            train_returns = calculate_returns(train_prices)
            last_price = float(train_prices.iloc[-1])  # ç¡®ä¿æ˜¯æ ‡é‡

            # è·å–å®é™…æœªæ¥æ•°æ®
            future_dates = pd.date_range(start=prediction_start, end=end_date, freq='B')
            actual_data = get_stock_data(ticker, prediction_start, end_date)
            actual_data = actual_data.reindex(future_dates).ffill()

            # æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹
            predictions = train_and_predict_garch(train_returns, last_price, len(future_dates))

            # æ•°æ®å¯¹é½
            min_len = min(len(predictions), len(actual_data))
            predictions = predictions[:min_len]
            actual_values = actual_data.values[:min_len]
            dates = future_dates[:min_len]

            # ç¡®ä¿æ•°æ®æ˜¯ä¸€ç»´çš„
            if isinstance(predictions, pd.Series):
                predictions = predictions.values
            if isinstance(predictions, pd.DataFrame):
                predictions = predictions.values
            predictions = np.array(predictions).flatten()

            if isinstance(actual_values, pd.Series):
                actual_values = actual_values.values
            if isinstance(actual_values, pd.DataFrame):
                actual_values = actual_values.values
            actual_values = np.array(actual_values).flatten()

            # ä¿å­˜å½“å‰é¢„æµ‹ç»“æœç”¨äºå¯¹æ¯”å›¾
            all_predictions[period_name] = {
                'dates': dates,
                'actual': actual_values,
                'predicted': predictions
            }

            # ä¿å­˜é¢„æµ‹ç»“æœ
            base_path = f'/Users/quyou/Desktop/æ¯•ä¸šè®¾è®¡/forecast data/GARCH/GOOG_garch_{period_name}'
            results_df = save_results(
                dates,
                actual_values,
                predictions,
                f'{base_path}.csv'
            )

            # ç»˜åˆ¶å•ä¸ªé¢„æµ‹ç»“æœ
            plot_predictions(
                dates,
                actual_values,
                predictions,
                f'GARCH Stock Price Prediction ({period_name} Training)',
                f'{base_path}_plot.png'
            )










            
****************************************LSTM*********************************
#ä»…å±•ç¤ºåŸºäºè¿‡å»ä¸‰å¹´çš„ä»£ç ï¼Œå…¶ä»–åŒç†








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import yfinance as yf
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from datetime import timedelta

# **1ï¸âƒ£ å‚æ•°è®¾ç½®**
ticker = "GOOG"
end_date = "2025-01-01"
train_days = 3 * 365  # è¿‡å» 3 å¹´ï¼ˆè®­ç»ƒæ•°æ®ï¼‰
future_steps = 21  # é¢„æµ‹æœªæ¥ 21 å¤©ï¼ˆå¯¹åº”2024å¹´12æœˆ2æ—¥è‡³12æœˆ31æ—¥ï¼‰
sequence_length = 180  # ä½¿ç”¨è¿‡å» 180 å¤©æ•°æ®é¢„æµ‹æœªæ¥ 21 å¤©

# **2ï¸âƒ£ æ•°æ®ä¸‹è½½**
data = yf.download(ticker, start="2019-01-01", end=end_date)

# **3ï¸âƒ£ é€‰æ‹©æ”¶ç›˜ä»·å¹¶æ¸…æ´—æ•°æ®**
data = data[['Close']].dropna()

# **4ï¸âƒ£ å¢åŠ æŠ€æœ¯æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰**
data['MA_30'] = data['Close'].rolling(window=30).mean()  # 30æ—¥ç§»åŠ¨å¹³å‡
data['RSI'] = 100 - (100 / (1 + (data['Close'].diff().gt(0).rolling(window=14).sum() / 14)))  # 14æ—¥RSI
data = data.dropna()

# **5ï¸âƒ£ æ•°æ®å½’ä¸€åŒ–**
scaler = MinMaxScaler(feature_range=(0, 1))
data_scaled = scaler.fit_transform(data)

# **6ï¸âƒ£ ç”Ÿæˆè®­ç»ƒæ•°æ®**
X, y = [], []
for i in range(len(data_scaled) - sequence_length - future_steps):
    X.append(data_scaled[i : i + sequence_length])
    y.append(data_scaled[i + sequence_length : i + sequence_length + future_steps, 0])  # å–ç¬¬ä¸€åˆ—ï¼ˆæ”¶ç›˜ä»·ï¼‰

X, y = np.array(X), np.array(y)

# **7ï¸âƒ£ æ•°æ®æ£€æŸ¥**
if X.shape[0] == 0:
    raise ValueError("æ•°æ®ä¸è¶³ï¼Œæ— æ³•æ„å»ºè®­ç»ƒé›†ï¼")

# **8ï¸âƒ£ æ„å»º LSTM æ¨¡å‹**
model = Sequential([
    LSTM(128, return_sequences=False, input_shape=(sequence_length, X.shape[2])),  # è¿™é‡Œè¿”å›åºåˆ—è®¾ç½®ä¸º False
    Dropout(0.2),
    Dense(future_steps)  # é¢„æµ‹æœªæ¥ 21 å¤©
])

model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')

# **9ï¸âƒ£ æå‰åœæ­¢**
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# **ğŸ”Ÿ è®­ç»ƒ LSTM æ¨¡å‹**
model.fit(X, y, epochs=100, batch_size=16, validation_split=0.2, callbacks=[early_stopping], verbose=1)

# **1ï¸âƒ£1ï¸âƒ£ è¿›è¡Œé¢„æµ‹**
last_sequence = data_scaled[-sequence_length:].reshape(1, sequence_length, X.shape[2])
future_predictions = model.predict(last_sequence)

# **1ï¸âƒ£2ï¸âƒ£ è¿›è¡Œé€†å½’ä¸€åŒ–**
# å°†é¢„æµ‹ç»“æœé‡æ–°è°ƒæ•´ä¸ºäºŒç»´æ•°ç»„ (21, 1)ï¼Œä¸ scaler å…¼å®¹
future_predictions = future_predictions.reshape(-1, 1)

# åªä½¿ç”¨æ”¶ç›˜ä»·è¿›è¡Œé€†å½’ä¸€åŒ–
# æ„é€ ä¸€ä¸ªä¸è¾“å…¥æ•°æ®ç›¸åŒçš„äºŒç»´æ•°ç»„ï¼Œå…¶ä¸­å…¶ä»–åˆ—ä¿æŒä¸ºé›¶
predictions_for_scaling = np.zeros((future_predictions.shape[0], data_scaled.shape[1]))  # ä¿æŒä¸æ•°æ®ç›¸åŒçš„åˆ—æ•°
predictions_for_scaling[:, 0] = future_predictions.flatten()  # åªå¡«å……ç¬¬ä¸€åˆ—ï¼ˆæ”¶ç›˜ä»·ï¼‰

# ä½¿ç”¨ scaler å¯¹é¢„æµ‹ç»“æœè¿›è¡Œé€†å½’ä¸€åŒ–
future_predictions = scaler.inverse_transform(predictions_for_scaling)[:, 0]

# **1ï¸âƒ£12ï¸âƒ£ è·å–çœŸå®æœªæ¥ä»·æ ¼ï¼ˆä¿®æ”¹æ—¥æœŸï¼‰**
future_dates = pd.to_datetime([
    '2024-12-02', '2024-12-03', '2024-12-04', '2024-12-05', '2024-12-06',
    '2024-12-09', '2024-12-10', '2024-12-11', '2024-12-12', '2024-12-13',
    '2024-12-16', '2024-12-17', '2024-12-18', '2024-12-19', '2024-12-20',
    '2024-12-23', '2024-12-24', '2024-12-25', '2024-12-26', '2024-12-27',
    '2024-12-30', '2024-12-31'
])[:21]  # åªå–å‰ 21 ä¸ªæ—¥æœŸ

# ä»Yahoo Financeä¸‹è½½å®é™…è‚¡ä»·
test_data = yf.download(ticker, start='2024-12-02', end='2024-12-31')['Close']

# å¦‚æœå®é™…æ•°æ®é•¿åº¦ä¸åŒ¹é…ï¼Œè¿›è¡Œå‰å‘å¡«å……
test_data = test_data.reindex(future_dates)  # å¯¹é½ç´¢å¼•
test_data = test_data.ffill()  # å‰å‘å¡«å……ç¼ºå¤±æ•°æ®

# **1ï¸âƒ£4ï¸âƒ£ ç¡®ä¿é¢„æµ‹æ•°æ®ä¸å®é™…æ•°æ®é•¿åº¦ä¸€è‡´**
assert len(future_predictions) == len(test_data), f"é¢„æµ‹æ•°æ®å’Œå®é™…æ•°æ®çš„é•¿åº¦ä¸ä¸€è‡´ï¼é¢„æµ‹é•¿åº¦ï¼š{len(future_predictions)}ï¼Œå®é™…æ•°æ®é•¿åº¦ï¼š{len(test_data)}"

# **1ï¸âƒ£5ï¸âƒ£ åˆ›å»º DataFrame**
forecast_df = pd.DataFrame({
    'Date': future_dates.strftime('%Y-%m-%d'),
    'Actual Price': test_data.values.flatten(),
    'Predicted Price': future_predictions
})

# **1ï¸âƒ£6ï¸âƒ£ ä¿å­˜ CSV**
forecast_file_path = "/Users/quyou/Desktop/æ¯•ä¸šè®¾è®¡/forecast data/LSTM/GOOG_lstm_3years_optimized_dec2024.csv"
forecast_df.to_csv(forecast_file_path, index=False)
print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜: {forecast_file_path}")

# **1ï¸âƒ£7ï¸âƒ£ ç»˜åˆ¶é¢„æµ‹ vs çœŸå®ä»·æ ¼**
plt.figure(figsize=(12, 6))
plt.plot(test_data.index, test_data.values, label='Actual Future Price', color='green')
plt.plot(test_data.index, future_predictions, label='Predicted Future Price', color='red', linestyle='dashed')
plt.title("LSTM Forecast vs Actual (GOOG) - Dec 2024 (Optimized)")
plt.xlabel("Date")
plt.ylabel("Price")
plt.legend()

# **1ï¸âƒ£8ï¸âƒ£ ä¿å­˜ç»˜å›¾**
plot_file_path = "/Users/quyou/Desktop/æ¯•ä¸šè®¾è®¡/forecast data/LSTM/GOOG_lstm_optimized_dec2024_comparison.png"
plt.savefig(plot_file_path)
plt.show()
print(f"âœ… å¯¹æ¯”å›¾å·²ä¿å­˜: {plot_file_path}")











************************************Transformer************************************
#ä»…å±•ç¤ºåŸºäºè¿‡å»ä¸‰å¹´çš„ä»£ç ï¼Œå…¶ä»–åŒç†









import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import yfinance as yf
import math
from torch.optim.lr_scheduler import CosineAnnealingLR


class StockDataset(Dataset):
    def __init__(self, data, sequence_length=60, prediction_length=30):
        self.data = data
        self.sequence_length = sequence_length
        self.prediction_length = prediction_length

    def __len__(self):
        return len(self.data) - self.sequence_length - self.prediction_length

    def __getitem__(self, idx):
        x = self.data[idx:idx + self.sequence_length]
        y = self.data[idx + self.sequence_length:idx + self.sequence_length + self.prediction_length, 0]
        return torch.FloatTensor(x), torch.FloatTensor(y)


class FastTransformerModel(nn.Module):
    def __init__(self, input_dim, d_model, nhead, num_layers, output_dim):
        super().__init__()
        self.input_projection = nn.Linear(input_dim, d_model)

        encoder_layers = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=512,
            dropout=0.1,
            activation='gelu',
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)

        self.decoder = nn.Sequential(
            nn.Linear(d_model, 256),
            nn.GELU(),
            nn.Dropout(0.1),
            nn.Linear(256, output_dim)
        )

    def forward(self, src):
        x = self.input_projection(src)
        transformer_out = self.transformer_encoder(x)
        pooled = transformer_out.mean(dim=1)
        output = self.decoder(pooled)
        return output


def add_technical_indicators(df):
    """æ·»åŠ åŸºæœ¬æŠ€æœ¯æŒ‡æ ‡"""
    df['MA20'] = df['Close'].rolling(window=20).mean()
    df['RSI'] = 100 - (100 / (1 + (df['Close'].diff().gt(0).rolling(window=14).sum() / 14)))
    df['Price_Change'] = df['Close'].pct_change()

    df = df.ffill()
    df = df.bfill()
    return df


if __name__ == "__main__":
    # å‚æ•°è®¾ç½®
    ticker = "GOOG"
    end_date = "2025-01-01"
    prediction_start = "2024-12-02"
    train_days = 3 * 365
    sequence_length = 60
    future_steps = 30

    # ä¸‹è½½æ•°æ®
    data = yf.download(ticker, start="2019-01-01", end=end_date)
    data = data[['Close', 'High', 'Low', 'Open', 'Volume']].copy()

    # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
    data = add_technical_indicators(data)
    data = data.dropna()

    # æ•°æ®å½’ä¸€åŒ–
    scaler = MinMaxScaler()
    data_scaled = scaler.fit_transform(data)

    # åˆ›å»ºæ•°æ®é›†
    dataset = StockDataset(data_scaled, sequence_length, future_steps)
    train_size = int(0.8 * len(dataset))
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, len(dataset) - train_size])

    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=64)

    # åˆå§‹åŒ–æ¨¡å‹
    model = FastTransformerModel(
        input_dim=data.shape[1],
        d_model=128,
        nhead=4,
        num_layers=3,
        output_dim=future_steps
    )

    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    scheduler = CosineAnnealingLR(optimizer, T_max=10, eta_min=1e-6)

    # è®­ç»ƒå¾ªç¯
    num_epochs = 100
    best_val_loss = float('inf')
    patience = 10
    no_improve = 0

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for batch_x, batch_y in train_loader:
            optimizer.zero_grad()
            output = model(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()
            train_loss += loss.item()

        scheduler.step()

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                output = model(batch_x)
                val_loss += criterion(output, batch_y).item()

        avg_train_loss = train_loss / len(train_loader)
        avg_val_loss = val_loss / len(val_loader)

        print(f'Epoch {epoch + 1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')

        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), 'best_transformer_model.pth')
            no_improve = 0
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"\nEarly stopping triggered! No improvement for {patience} epochs")
                break

    # é¢„æµ‹
    model.load_state_dict(torch.load('best_transformer_model.pth'))
    model.eval()

    # ä¿®å¤ç»´åº¦é—®é¢˜
    prediction_data = data.loc[:prediction_start].iloc[-sequence_length:]
    prediction_scaled = scaler.transform(prediction_data)

    last_sequence = torch.FloatTensor(prediction_scaled).unsqueeze(0)
    with torch.no_grad():
        predictions = model(last_sequence).numpy().flatten()

    # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœ
    predictions_scaled = np.zeros((len(predictions), data.shape[1]))
    predictions_scaled[:, 0] = predictions
    predictions = scaler.inverse_transform(predictions_scaled)[:, 0]

    # ç”Ÿæˆé¢„æµ‹æ—¥æœŸ
    future_dates = pd.date_range(start=prediction_start, end="2024-12-31", freq='B')

    # è·å–çœŸå®ä»·æ ¼å¹¶ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„
    test_data = yf.download(ticker, start=prediction_start, end="2024-12-31")['Close']
    test_data = test_data.reindex(future_dates).ffill()
    test_data_values = test_data.values.flatten()  # ç¡®ä¿æ˜¯ä¸€ç»´æ•°ç»„

    # ç¡®ä¿é¢„æµ‹ç»“æœå’Œå®é™…å€¼é•¿åº¦ä¸€è‡´
    min_len = min(len(predictions), len(test_data_values))
    predictions = predictions[:min_len]
    test_data_values = test_data_values[:min_len]
    future_dates = future_dates[:min_len]

    # ä¿å­˜é¢„æµ‹ç»“æœ
    results_df = pd.DataFrame({
        'Date': [d.strftime('%Y-%m-%d') for d in future_dates],
        'Actual_Price': test_data_values,
        'Predicted_Price': predictions
    })
    results_df.to_csv('/Users/quyou/Desktop/æ¯•ä¸šè®¾è®¡/forecast data/Transformer/GOOG_transformer_3years_fast.csv',
                      index=False)

    # ç»˜åˆ¶ç»“æœ
    plt.figure(figsize=(12, 6))
    plt.plot(future_dates, test_data_values, label='Actual Price', color='green')
    plt.plot(future_dates, predictions, label='Predicted Price', color='red', linestyle='--')
    plt.title('Fast Transformer Stock Price Prediction (Dec 2024)')
    plt.xlabel('Date')
    plt.ylabel('Price')
    plt.legend()
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    mse = np.mean((test_data_values - predictions) ** 2)
    rmse = np.sqrt(mse)
    mae = np.mean(np.abs(test_data_values - predictions))
    mape = np.mean(np.abs((test_data_values - predictions) / test_data_values)) * 100

    print("\nModel Performance Metrics:")
    print(f"MSE: {mse:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAE: {mae:.2f}")
